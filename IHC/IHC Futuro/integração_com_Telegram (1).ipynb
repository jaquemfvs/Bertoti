{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE15mtXLKbmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47cb837-d5c0-4552-9e56-58b98add4f86",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama_cpp_python in /usr/local/lib/python3.10/dist-packages (0.2.77)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_cpp_python) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama_cpp_python) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama_cpp_python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama_cpp_python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama_cpp_python) (2.1.5)\n",
            "Requirement already satisfied: pytelegrambotapi in /usr/local/lib/python3.10/dist-packages (4.18.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytelegrambotapi) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytelegrambotapi) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama_cpp_python\n",
        "!pip install pytelegrambotapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEOAGxcJKdm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "788422df-9f46-4cb3-88be-dca022ec4221",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.cache/huggingface/hub/models--meetkai--functionary-small-v2.5-GGUF/snapshots/37e7f0678f49a1d360090d0f4b79d138ca2248ee/./functionary-small-v2.5.Q4_0.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = llama3-functionary-hf\n",
            "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_0:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: n_ctx_train      = 8192\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_0\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) \n",
            "llm_load_print_meta: general.name     = llama3-functionary-hf\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
            "llm_load_tensors:        CPU buffer size =  4437.80 MiB\n",
            ".......................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' or message['role'] == 'system' %}\\n{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + message['content'] + '<|eot_id|>' }}{% elif message['role'] == 'tool' %}\\n{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' + 'name=' + message['name'] + '\\n' + message['content'] + '<|eot_id|>' }}{% else %}\\n{{ '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'}}{% if message['content'] is not none %}\\n{{ message['content'] }}{% endif %}\\n{% if 'tool_calls' in message and message['tool_calls'] is not none %}\\n{% for tool_call in message['tool_calls'] %}\\n{{ '<|reserved_special_token_249|>' + tool_call['function']['name'] + '\\n' + tool_call['function']['arguments'] }}{% endfor %}\\n{% endif %}\\n{{ '<|eot_id|>' }}{% endif %}\\n{% endfor %}\\n{% if add_generation_prompt %}{{ '<|start_header_id|>{role}<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'llama3-functionary-hf', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '2', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
            "Available chat formats from metadata: chat_template.default\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import sqlite3\n",
        "from llama_cpp import Llama\n",
        "from llama_cpp.llama_tokenizer import LlamaHFTokenizer\n",
        "\n",
        "llm = Llama.from_pretrained(\n",
        "    repo_id=\"meetkai/functionary-small-v2.5-GGUF\",\n",
        "    filename=\"functionary-small-v2.5.Q4_0.gguf\",\n",
        "    chat_format=\"functionary-v2\",\n",
        "    tokenizer=LlamaHFTokenizer.from_pretrained(\"meetkai/functionary-small-v2.5-GGUF\"),\n",
        "    n_gpu_layers=-1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funções da aplicação"
      ],
      "metadata": {
        "id": "6xiYn5eDdHk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "global pedido\n",
        "pedido = {'Cliente': '', 'itens': [], 'valor': '', 'status': 'In progress', 'forma_pagamento': ''}\n",
        "\n",
        "def calcular_total():\n",
        "    valor = 0\n",
        "    for item in pedido['itens']:\n",
        "        valor += item['valor']\n",
        "    pedido['valor'] = valor\n",
        "\n",
        "#Dar função itens_disponiveis() no enum\n",
        "def adicionar_item(item):\n",
        "    conexao = sqlite3.connect('cardapio.db')\n",
        "    cursor = conexao.cursor()\n",
        "    cursor.execute('SELECT * FROM cardapio WHERE nome = ?', (item,))\n",
        "    item_info = cursor.fetchone()\n",
        "    if item_info:\n",
        "        novo_item = {'nome': item_info[1], 'tipo': item_info[2], 'valor': item_info[3]}\n",
        "        pedido['itens'].append(novo_item)\n",
        "    conexao.close()\n",
        "    return f\"Great! I just added {item} to your order!\"\n",
        "\n",
        "#Dar função itens_pedido() no enum\n",
        "def remover_item(item):\n",
        "    for i in range(len(pedido['itens'])):\n",
        "        if pedido['itens'][i]['nome'] == item:\n",
        "            pedido['itens'].pop(i)\n",
        "            break\n",
        "    return f\"Ok! I just removed {item} from your order!\"\n",
        "\n",
        "def calcular_valor():\n",
        "    calcular_total()\n",
        "    return f\"The total value of your order is ${valor}\"\n",
        "\n",
        "def finalizar_pedido():\n",
        "    calcular_total()\n",
        "    if pedido['Cliente'] != '' and pedido['forma_pagamento'] != '' and pedido['itens'] != []:\n",
        "        pedido['status'] = 'Finalized'\n",
        "        return f\"Order finalized! I just sent it to our kitchen team. Thank you for ordering with us!\"\n",
        "    return f\"Oops! It seems like some information is missing to finalize the order. Please check if you provided your name, payment method, and the items in the order.\"\n",
        "\n",
        "def cancelar_pedido():\n",
        "    pedido['status'] = 'Cancelled'\n",
        "    return f\"Order cancelled! If you need anything else, just let me know!\"\n",
        "\n",
        "#Dar Array com formas de pagamento\n",
        "def forma_pagamento(forma):\n",
        "    pedido['forma_pagamento'] = forma\n",
        "    return f\"Chosen payment method: {forma}\"\n",
        "\n",
        "def informacoes_pedido():\n",
        "    calcular_total()\n",
        "    return f\"Here are the details of your order so far:\\nCustomer: {pedido['Cliente']}\\nItems: {itens_pedido_print()}\\nValue: {pedido['valor']}\\nStatus: {pedido['status']}\\nPayment Method: {pedido['forma_pagamento']}\"\n",
        "\n",
        "def itens_pedido():\n",
        "    itens = []\n",
        "    for item in pedido['itens']:\n",
        "        itens.append(item['nome'])\n",
        "    return itens\n",
        "\n",
        "def itens_pedido_print():\n",
        "    resp = 'Items in your order:\\n'\n",
        "    for item in pedido['itens']:\n",
        "        resp = resp + f\"Name: {item['nome']}\\nType: {item['tipo']}\\nPrice: {item['valor']}\\n\"\n",
        "    return resp\n",
        "\n",
        "def itens_disponiveis():\n",
        "    conexao = sqlite3.connect('cardapio.db')\n",
        "    cursor = conexao.cursor()\n",
        "    cursor.execute('SELECT * FROM cardapio WHERE disponibilidade = 1')\n",
        "    itens = cursor.fetchall()\n",
        "    disponiveis = []\n",
        "    for i in itens:\n",
        "        disponiveis.append(i[1])\n",
        "    conexao.close()\n",
        "    return disponiveis\n",
        "\n",
        "def itens_disponiveis_print():\n",
        "    resp = 'This is our menu:\\n'\n",
        "    conexao = sqlite3.connect('cardapio.db')\n",
        "    cursor = conexao.cursor()\n",
        "    cursor.execute('SELECT * FROM cardapio WHERE disponibilidade = 1')\n",
        "    itens = cursor.fetchall()\n",
        "    for item in itens:\n",
        "        resp = resp + f\"Name: {item[1]}\\nPrice: {item[3]}\\nType: {item[2]}\\n\"\n",
        "    conexao.close()\n",
        "    return resp\n",
        "\n",
        "def adicionar_cliente(nome):\n",
        "    pedido['Cliente'] = nome\n",
        "    return f\"Great! {nome}, I just added your name to the order!\"\n",
        "\n",
        "def iniciar_pedido():\n",
        "    pedido = {'Cliente': '', 'itens': [], 'valor': '', 'status': 'In progress', 'forma_pagamento': ''}\n",
        "    return '''Order started!\n",
        "    To finalize your order, I will need some information.\n",
        "    Your name, what you want to order, and the payment method.\n",
        "    '''\n",
        "\n",
        "def iniciar():\n",
        "    return '''Welcome to the ordering system!\n",
        "    How can I assist you?\n",
        "    Ask me what delicious items are on our menu!\n",
        "    Tell me what you would like to order!\n",
        "    I can help you with your order!\n",
        "    '''"
      ],
      "metadata": {
        "id": "t_m3ukPOdMKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrijFQtp9nJh"
      },
      "outputs": [],
      "source": [
        "#------------------------------------Funções Para o telegram-------------------------------------------------------\n",
        "\n",
        "def question(s):\n",
        "  messages[0][\"content\"] = s\n",
        "\n",
        "def response():\n",
        "  try:\n",
        "    result = llm.create_chat_completion(\n",
        "        messages = messages,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",\n",
        "    )\n",
        "  except:\n",
        "    print('Error')\n",
        "    return 'Sorry, it seems an error has occurred, please try again later.'\n",
        "  #s = result[\"choices\"][0][\"message\"][\"function_call\"]\n",
        "\n",
        "  print(f'Print: {result}')\n",
        "  s = result['choices'][0]['message']['tool_calls'][0]['function']['name']\n",
        "\n",
        "\n",
        "  content = s.split('functions.')[1].split('\\n')[0]\n",
        "  if '{}' in s:\n",
        "    functionCall = f\"{content}()\"\n",
        "    print(functionCall)\n",
        "    return eval(functionCall)\n",
        "  value = s.split(': \"')[1].split('\"}')[0]\n",
        "  functionCall = f\"{content}('{value}')\"\n",
        "  print(functionCall)\n",
        "  return eval(functionCall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AEA9vD0BnaS"
      },
      "source": [
        "Banco de Dados Exemplo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Conectar-se ao banco de dados (se não existir, será criado automaticamente)\n",
        "conexao = sqlite3.connect('cardapio.db')\n",
        "\n",
        "# Criar um cursor\n",
        "cursor = conexao.cursor()\n",
        "\n",
        "# Criar uma tabela\n",
        "cursor.execute('''CREATE TABLE IF NOT EXISTS cardapio (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    nome TEXT,\n",
        "    tipo TEXT,\n",
        "    preco FLOAT,\n",
        "    disponibilidade BOOLEAN\n",
        "    )''')\n",
        "\n",
        "# Inserir dados na tabela\n",
        "menu_items = [\n",
        "    ('Classic Burger', 'Burger', 8.99, 1),\n",
        "    ('Cheeseburger', 'Burger', 9.99, 1),\n",
        "    ('Mushroom Swiss Burger', 'Burger', 10.49, 0),\n",
        "    ('Veggie Burger', 'Burger', 9.49, 1),\n",
        "    ('Turkey Burger', 'Burger', 9.99, 0),\n",
        "    ('Chicken Sandwich', 'Sandwich', 9.99, 1),\n",
        "    ('Onion Rings', 'Side', 4.99, 1),\n",
        "    ('Coke', 'Drink', 2.99, 1),\n",
        "    ('Sprite', 'Drink', 2.99, 0),\n",
        "    ('Iced Tea', 'Drink', 2.99, 1),\n",
        "    ('French Fries', 'Side', 3.99, 1),\n",
        "]\n",
        "\n",
        "# Insert data using executemany\n",
        "cursor.executemany(\"INSERT INTO cardapio (nome, tipo, preco, disponibilidade) VALUES (?, ?, ?, ?)\", menu_items)\n",
        "\n",
        "\n",
        "# Executar uma consulta SQL\n",
        "cursor.execute(\"SELECT * FROM cardapio\")\n",
        "print(cursor.fetchall())  # Exibir todos os resultados\n",
        "\n",
        "# Commit e fechar a conexão\n",
        "conexao.commit()\n",
        "conexao.close()"
      ],
      "metadata": {
        "id": "kI1aQhtVEy5O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3336659-05b6-49e4-86e2-c0d80a533871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 'Classic Burger', 'Burger', 8.99, 1), (2, 'Cheeseburger', 'Burger', 9.99, 1), (3, 'Mushroom Swiss Burger', 'Burger', 10.49, 0), (4, 'Veggie Burger', 'Burger', 9.49, 1), (5, 'Turkey Burger', 'Burger', 9.99, 0), (6, 'Chicken Sandwich', 'Sandwich', 9.99, 1), (7, 'Onion Rings', 'Side', 4.99, 1), (8, 'Coke', 'Drink', 2.99, 1), (9, 'Sprite', 'Drink', 2.99, 0), (10, 'Iced Tea', 'Drink', 2.99, 1), (11, 'French Fries', 'Side', 3.99, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conexao = sqlite3.connect('cardapio.db')\n",
        "\n",
        "# Criar um cursor\n",
        "cursor = conexao.cursor()\n",
        "\n",
        "cursor.execute('DROP TABLE cardapio')\n",
        "conexao.commit()\n",
        "conexao.close()"
      ],
      "metadata": {
        "id": "TRl9NAU3yloL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Conectar-se ao banco de dados (se não existir, será criado automaticamente)\n",
        "conexao = sqlite3.connect('cardapio.db')\n",
        "\n",
        "# Criar um cursor\n",
        "cursor = conexao.cursor()\n",
        "\n",
        "# Criar uma tabela\n",
        "cursor.execute('''CREATE TABLE IF NOT EXISTS cardapio (\n",
        "    id INTEGER PRIMARY KEY,\n",
        "    nome TEXT,\n",
        "    tipo TEXT,\n",
        "    preco FLOAT,\n",
        "    disponibilidade BOOLEAN\n",
        "    )''')\n",
        "\n",
        "# Inserir dados na tabela\n",
        "menu_items = [\n",
        "    ('Classic Burger', 'Burger', 8.99, 1),\n",
        "    ('Cheeseburger', 'Burger', 9.99, 1),\n",
        "    ('Mushroom Swiss Burger', 'Burger', 10.49, 0),\n",
        "    ('Veggie Burger', 'Burger', 9.49, 1),\n",
        "    ('Turkey Burger', 'Burger', 9.99, 0),\n",
        "    ('Chicken Sandwich', 'Sandwich', 9.99, 1),\n",
        "    ('Onion Rings', 'Side', 4.99, 1),\n",
        "    ('Coke', 'Drink', 2.99, 1),\n",
        "    ('Sprite', 'Drink', 2.99, 0),\n",
        "    ('Iced Tea', 'Drink', 2.99, 1),\n",
        "    ('French Fries', 'Side', 3.99, 1),\n",
        "]\n",
        "\n",
        "# Insert data using executemany\n",
        "cursor.executemany(\"INSERT INTO cardapio (nome, tipo, preco, disponibilidade) VALUES (?, ?, ?, ?)\", menu_items)\n",
        "\n",
        "\n",
        "# Executar uma consulta SQL\n",
        "cursor.execute(\"SELECT * FROM cardapio\")\n",
        "print(cursor.fetchall())  # Exibir todos os resultados\n",
        "\n",
        "# Commit e fechar a conexão\n",
        "conexao.commit()\n",
        "conexao.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3336659-05b6-49e4-86e2-c0d80a533871",
        "id": "EtF9vSVkj4oD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, 'Classic Burger', 'Burger', 8.99, 1), (2, 'Cheeseburger', 'Burger', 9.99, 1), (3, 'Mushroom Swiss Burger', 'Burger', 10.49, 0), (4, 'Veggie Burger', 'Burger', 9.49, 1), (5, 'Turkey Burger', 'Burger', 9.99, 0), (6, 'Chicken Sandwich', 'Sandwich', 9.99, 1), (7, 'Onion Rings', 'Side', 4.99, 1), (8, 'Coke', 'Drink', 2.99, 1), (9, 'Sprite', 'Drink', 2.99, 0), (10, 'Iced Tea', 'Drink', 2.99, 1), (11, 'French Fries', 'Side', 3.99, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psu7BSZfBpYK"
      },
      "source": [
        "fim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDPXod-UKkgf"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"what is 75 minus 13?\"}\n",
        "]\n",
        "\n",
        "tools = [\n",
        "    #Função adicionar_item(item)\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"adicionar_item\",\n",
        "            \"description\": \"Add item to the current order.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"item\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The name of the item that will be added to the order.\",\n",
        "                        \"enum\": itens_disponiveis()\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"item\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    #Função remover_item(item)\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"remover_item\",\n",
        "            \"description\": \"Remove item from the current order.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"item\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The name of the item that will be removed from the order.\",\n",
        "                        \"enum\": itens_pedido()\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"item\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    #Função calcular_valor()\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"calcular_valor\",\n",
        "            \"description\": \"Calculate the total value of the current order and return the result.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    #Função finalizar_pedido()\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"finalizar_pedido\",\n",
        "            \"description\": \"Finalize the current order and send it to the kitchen team.\",\n",
        "            \"parameters\": {\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    #Função cancelar_pedido()\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"cancelar_pedido\",\n",
        "            \"description\": \"Cancel the current order.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    #Função forma_pagamento(forma)\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"forma_pagamento\",\n",
        "            \"description\": \"Choose the payment method for the current order.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"forma\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The payment method chosen by the customer.\",\n",
        "                        \"enum\": ['Credit Card', 'Debit Card', 'Cash']\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"forma\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    #Função informacoes_pedido()\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"informacoes_pedido\",\n",
        "            \"description\": \"Return the details of the current order.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    #Função itens_pedido_print()\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"itens_pedido_print\",\n",
        "            \"description\": \"Return the items in the current order.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    #Função itens_disponiveis_print()\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"itens_disponiveis_print\",\n",
        "            \"description\": \"Return the items available in the menu.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    #Função adicionar_cliente(nome)\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"adicionar_cliente\",\n",
        "            \"description\": \"Add the customer's name to the current order.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"nome\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The name of the customer\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"nome\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    #Função iniciar_pedido()\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"iniciar_pedido\",\n",
        "            \"description\": \"Start a new order.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESncf9PDCa7W"
      },
      "outputs": [],
      "source": [
        "import telebot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6umXZI0eCfIS"
      },
      "outputs": [],
      "source": [
        "API_TOKEN = 'You API bot Token'\n",
        "bot = telebot.TeleBot(API_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@bot.message_handler(func=lambda message: True, content_types=['text'])\n",
        "def reply_hi(message):\n",
        "  sent_msg = bot.send_message(message.chat.id, iniciar())\n",
        "  bot.register_next_step_handler(sent_msg, text_response)\n",
        "\n",
        "def text_response(pm):\n",
        "  question(pm.text)\n",
        "  sent_msg = bot.reply_to(pm, response())\n",
        "  bot.register_next_step_handler(sent_msg, text_response)\n",
        "\n",
        "\n",
        "bot.polling()"
      ],
      "metadata": {
        "id": "BW35kMTlwze0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669a4654-985a-4acb-aad2-5e5a9195d50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =  216200.04 ms\n",
            "llama_print_timings:      sample time =      17.57 ms /     7 runs   (    2.51 ms per token,   398.41 tokens per second)\n",
            "llama_print_timings: prompt eval time =  216199.31 ms /   438 tokens (  493.61 ms per token,     2.03 tokens per second)\n",
            "llama_print_timings:        eval time =    6872.72 ms /     6 runs   ( 1145.45 ms per token,     0.87 tokens per second)\n",
            "llama_print_timings:       total time =  223173.67 ms /   444 tokens\n",
            "from_string grammar:\n",
            "root ::= object \n",
            "object ::= [{] ws object_11 [}] ws \n",
            "value ::= object | array | string | number | value_6 ws \n",
            "array ::= [[] ws array_15 []] ws \n",
            "string ::= [\"] string_18 [\"] ws \n",
            "number ::= number_19 number_25 number_29 ws \n",
            "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
            "ws ::= ws_31 \n",
            "object_8 ::= string [:] ws value object_10 \n",
            "object_9 ::= [,] ws string [:] ws value \n",
            "object_10 ::= object_9 object_10 | \n",
            "object_11 ::= object_8 | \n",
            "array_12 ::= value array_14 \n",
            "array_13 ::= [,] ws value \n",
            "array_14 ::= array_13 array_14 | \n",
            "array_15 ::= array_12 | \n",
            "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
            "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_18 ::= string_16 string_18 | \n",
            "number_19 ::= number_20 number_21 \n",
            "number_20 ::= [-] | \n",
            "number_21 ::= [0-9] | [1-9] number_22 \n",
            "number_22 ::= [0-9] number_22 | \n",
            "number_23 ::= [.] number_24 \n",
            "number_24 ::= [0-9] number_24 | [0-9] \n",
            "number_25 ::= number_23 | \n",
            "number_26 ::= [eE] number_27 number_28 \n",
            "number_27 ::= [-+] | \n",
            "number_28 ::= [0-9] number_28 | [0-9] \n",
            "number_29 ::= number_26 | \n",
            "ws_30 ::= [ <U+0009><U+000A>] ws \n",
            "ws_31 ::= ws_30 | \n",
            "\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to parse function body as JSON schema, falling back to default grammar\n",
            "'NoneType' object has no attribute 'get'\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =  216200.04 ms\n",
            "llama_print_timings:      sample time =      56.67 ms /     2 runs   (   28.33 ms per token,    35.29 tokens per second)\n",
            "llama_print_timings: prompt eval time =    2851.59 ms /     6 tokens (  475.26 ms per token,     2.10 tokens per second)\n",
            "llama_print_timings:        eval time =    1092.02 ms /     1 runs   ( 1092.02 ms per token,     0.92 tokens per second)\n",
            "llama_print_timings:       total time =    4033.76 ms /     7 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =  216200.04 ms\n",
            "llama_print_timings:      sample time =       2.88 ms /     1 runs   (    2.88 ms per token,   346.86 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1124.54 ms /     1 runs   ( 1124.54 ms per token,     0.89 tokens per second)\n",
            "llama_print_timings:       total time =    1143.85 ms /     1 tokens\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Print: {'id': 'chatcmpl-8ded8bd2-d9c6-43a0-b2bb-6195a65bd368', 'object': 'chat.completion', 'created': 1717590540, 'model': '/root/.cache/huggingface/hub/models--meetkai--functionary-small-v2.5-GGUF/snapshots/37e7f0678f49a1d360090d0f4b79d138ca2248ee/./functionary-small-v2.5.Q4_0.gguf', 'choices': [{'index': 0, 'logprobs': None, 'message': {'role': 'assistant', 'content': None, 'tool_calls': [{'id': 'call_a1f1TrQHrTQzkqLE8Y4QTUjm', 'type': 'function', 'function': {'name': 'functions.iniciar_pedido\\n{}', 'arguments': '{}'}}]}, 'finish_reason': 'tool_calls'}], 'usage': {'prompt_tokens': 450, 'completion_tokens': 7, 'total_tokens': 450}}\n",
            "iniciar_pedido()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =  216200.04 ms\n",
            "llama_print_timings:      sample time =      52.75 ms /    11 runs   (    4.80 ms per token,   208.55 tokens per second)\n",
            "llama_print_timings: prompt eval time =   20757.08 ms /    19 tokens ( 1092.48 ms per token,     0.92 tokens per second)\n",
            "llama_print_timings:        eval time =   22766.38 ms /    10 runs   ( 2276.64 ms per token,     0.44 tokens per second)\n",
            "llama_print_timings:       total time =   43853.52 ms /    29 tokens\n",
            "from_string grammar:\n",
            "root ::= object \n",
            "object ::= [{] ws object_11 [}] ws \n",
            "value ::= object | array | string | number | value_6 ws \n",
            "array ::= [[] ws array_15 []] ws \n",
            "string ::= [\"] string_18 [\"] ws \n",
            "number ::= number_19 number_25 number_29 ws \n",
            "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
            "ws ::= ws_31 \n",
            "object_8 ::= string [:] ws value object_10 \n",
            "object_9 ::= [,] ws string [:] ws value \n",
            "object_10 ::= object_9 object_10 | \n",
            "object_11 ::= object_8 | \n",
            "array_12 ::= value array_14 \n",
            "array_13 ::= [,] ws value \n",
            "array_14 ::= array_13 array_14 | \n",
            "array_15 ::= array_12 | \n",
            "string_16 ::= [^\"\\<U+0000>-<U+001F>] | [\\] string_17 \n",
            "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
            "string_18 ::= string_16 string_18 | \n",
            "number_19 ::= number_20 number_21 \n",
            "number_20 ::= [-] | \n",
            "number_21 ::= [0-9] | [1-9] number_22 \n",
            "number_22 ::= [0-9] number_22 | \n",
            "number_23 ::= [.] number_24 \n",
            "number_24 ::= [0-9] number_24 | [0-9] \n",
            "number_25 ::= number_23 | \n",
            "number_26 ::= [eE] number_27 number_28 \n",
            "number_27 ::= [-+] | \n",
            "number_28 ::= [0-9] number_28 | [0-9] \n",
            "number_29 ::= number_26 | \n",
            "ws_30 ::= [ <U+0009><U+000A>] ws \n",
            "ws_31 ::= ws_30 | \n",
            "\n",
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to parse function body as JSON schema, falling back to default grammar\n",
            "'NoneType' object has no attribute 'get'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =  216200.04 ms\n",
            "llama_print_timings:      sample time =      72.27 ms /     2 runs   (   36.13 ms per token,    27.67 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4007.30 ms /     6 tokens (  667.88 ms per token,     1.50 tokens per second)\n",
            "llama_print_timings:        eval time =     970.99 ms /     1 runs   (  970.99 ms per token,     1.03 tokens per second)\n",
            "llama_print_timings:       total time =    5083.69 ms /     7 tokens\n",
            "Llama.generate: prefix-match hit\n",
            "\n",
            "llama_print_timings:        load time =  216200.04 ms\n",
            "llama_print_timings:      sample time =       2.46 ms /     1 runs   (    2.46 ms per token,   406.83 tokens per second)\n",
            "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
            "llama_print_timings:        eval time =    1023.76 ms /     1 runs   ( 1023.76 ms per token,     0.98 tokens per second)\n",
            "llama_print_timings:       total time =    1046.02 ms /     1 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Print: {'id': 'chatcmpl-3e1ee1e4-22f7-474f-97eb-0f6e9e23f009', 'object': 'chat.completion', 'created': 1717593837, 'model': '/root/.cache/huggingface/hub/models--meetkai--functionary-small-v2.5-GGUF/snapshots/37e7f0678f49a1d360090d0f4b79d138ca2248ee/./functionary-small-v2.5.Q4_0.gguf', 'choices': [{'index': 0, 'logprobs': None, 'message': {'role': 'assistant', 'content': None, 'tool_calls': [{'id': 'call_oT97gGTjxTWpLuGh3CKjToMh', 'type': 'function', 'function': {'name': 'functions.itens_disponiveis_print\\n{}', 'arguments': '{}'}}]}, 'finish_reason': 'tool_calls'}], 'usage': {'prompt_tokens': 455, 'completion_tokens': 11, 'total_tokens': 455}}\n",
            "itens_disponiveis_print()\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}